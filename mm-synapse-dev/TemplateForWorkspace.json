{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "mm-synapse-dev"
		},
		"mm-synapse-dev-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'mm-synapse-dev-WorkspaceDefaultSqlServer'"
		},
		"LS_Datalake_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://mmxsynapsexdev.dfs.core.windows.net/"
		},
		"LS_KeyVault_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://mm-kv-dev.vault.azure.net/"
		},
		"Ls_Rest_MelParkSensors_01_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://data.melbourne.vic.gov.au/resource/"
		},
		"mm-synapse-dev-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://mmxsynapsexdev.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/pool002')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/P_Ingest_MelbParkingData')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Set infilefolder",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "infilefolder",
							"value": {
								"value": "@utcnow('yyyy_MM_dd_hh_mm_ss')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "DownloadSensorData",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Set infilefolder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET"
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "Ds_REST_MelbParkingData",
								"type": "DatasetReference",
								"parameters": {
									"relativeurl": "dtpv-d4pf.json"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Ds_AdlsGen2_MelbParkingData",
								"type": "DatasetReference",
								"parameters": {
									"infilefolder": "@variables('infilefolder')",
									"infilename": "MelbParkingSensorData.json",
									"container": "datalake/data/lnd"
								}
							}
						]
					},
					{
						"name": "DownloadBayData",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Set infilefolder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET"
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "Ds_REST_MelbParkingData",
								"type": "DatasetReference",
								"parameters": {
									"relativeurl": "wuf8-susg.json"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Ds_AdlsGen2_MelbParkingData",
								"type": "DatasetReference",
								"parameters": {
									"infilefolder": "@variables('infilefolder')",
									"infilename": "MelbParkingBayData.json",
									"container": "datalake/data/lnd"
								}
							}
						]
					},
					{
						"name": "StandardizeData",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "DownloadSensorData",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "DownloadBayData",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "02_standardize",
								"type": "NotebookReference"
							},
							"parameters": {
								"infilefolder": {
									"value": {
										"value": "@variables('infilefolder')",
										"type": "Expression"
									},
									"type": "string"
								},
								"loadid": {
									"value": {
										"value": "@pipeline().RunId",
										"type": "Expression"
									},
									"type": "string"
								},
								"pipelinename": {
									"value": {
										"value": "@pipeline().Pipeline",
										"type": "Expression"
									},
									"type": "string"
								},
								"keyvaultlsname": {
									"value": "LS_KeyVault",
									"type": "string"
								},
								"adls2lsname": {
									"value": "LS_Datalake",
									"type": "string"
								}
							},
							"sparkPool": {
								"referenceName": "pool001",
								"type": "BigDataPoolReference"
							}
						}
					},
					{
						"name": "TransformData",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "StandardizeData",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "03_transform",
								"type": "NotebookReference"
							},
							"parameters": {
								"loadid": {
									"value": {
										"value": "@pipeline().RunId",
										"type": "Expression"
									},
									"type": "string"
								},
								"pipelinename": {
									"value": {
										"value": "@pipeline().Pipeline",
										"type": "Expression"
									},
									"type": "string"
								},
								"keyvaultlsname": {
									"value": "LS_KeyVault",
									"type": "string"
								}
							},
							"sparkPool": {
								"referenceName": "pool001",
								"type": "BigDataPoolReference"
							}
						}
					},
					{
						"name": "Load SQL dedicated pool",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "TransformData",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "samplededicated",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "dbo.load_dw",
							"storedProcedureParameters": {
								"load_id": {
									"value": {
										"value": "@pipeline().RunId",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"infilefolder": {
						"type": "String",
						"defaultValue": "Ind"
					}
				},
				"annotations": [],
				"lastPublishTime": "2021-10-26T23:43:06Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Ds_REST_MelbParkingData')]",
				"[concat(variables('workspaceId'), '/datasets/Ds_AdlsGen2_MelbParkingData')]",
				"[concat(variables('workspaceId'), '/notebooks/02_standardize')]",
				"[concat(variables('workspaceId'), '/bigDataPools/pool001')]",
				"[concat(variables('workspaceId'), '/notebooks/03_transform')]",
				"[concat(variables('workspaceId'), '/sqlPools/samplededicated')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Ds_AdlsGen2_MelbParkingData')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_Datalake",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"infilefolder": {
						"type": "string"
					},
					"infilename": {
						"type": "string"
					},
					"container": {
						"type": "string",
						"defaultValue": "datalake/data/lnd"
					}
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().infilename",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@concat(dataset().container, '/', dataset().infilefolder)",
							"type": "Expression"
						}
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_Datalake')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Ds_REST_MelbParkingData')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Ls_Rest_MelParkSensors_01",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"relativeurl": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@dataset().relativeurl",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Ls_Rest_MelParkSensors_01')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SiteInfo')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "Site",
						"type": "nvarchar"
					},
					{
						"name": "SiteName",
						"type": "nvarchar"
					},
					{
						"name": "SiteLocation",
						"type": "nvarchar"
					},
					{
						"name": "State",
						"type": "nvarchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "siteinfo"
				},
				"sqlPool": {
					"referenceName": "samplededicated",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/samplededicated')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_Datalake')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('LS_Datalake_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_KeyVault')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('LS_KeyVault_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Ls_Rest_MelParkSensors_01')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('Ls_Rest_MelParkSensors_01_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mm-synapse-dev-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('mm-synapse-dev-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mm-synapse-dev-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('mm-synapse-dev-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/001_CreateUser')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "DedicatedSQLPool"
				},
				"content": {
					"query": "--https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/load-data-from-azure-blob-storage-using-copy\n--run on dedicated sql pool on master\n--another sample-\n--anpother\nCREATE LOGIN LoaderRC20 WITH PASSWORD = 'a123STRONGpassword!';\nCREATE USER LoaderRC20 FOR LOGIN LoaderRC20;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "samplededicated"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/002_createdatabaseuser')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "DedicatedSQLPool"
				},
				"content": {
					"query": "-- run on samplededicated on database samplededicated\nCREATE USER LoaderRC20 FOR LOGIN LoaderRC20;\nGRANT CONTROL ON DATABASE::[samplededicated] to LoaderRC20;\nEXEC sp_addrolemember 'staticrc20', 'LoaderRC20';",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "samplededicated",
						"poolName": "samplededicated"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/003_createtables')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "DedicatedSQLPool"
				},
				"content": {
					"query": "CREATE TABLE [dbo].[Date]\n(\n    [DateID] int NOT NULL,\n    [Date] datetime NULL,\n    [DateBKey] char(10) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [DayOfMonth] varchar(2) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [DaySuffix] varchar(4) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [DayName] varchar(9) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [DayOfWeek] char(1) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [DayOfWeekInMonth] varchar(2) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [DayOfWeekInYear] varchar(2) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [DayOfQuarter] varchar(3) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [DayOfYear] varchar(3) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [WeekOfMonth] varchar(1) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [WeekOfQuarter] varchar(2) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [WeekOfYear] varchar(2) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [Month] varchar(2) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [MonthName] varchar(9) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [MonthOfQuarter] varchar(2) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [Quarter] char(1) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [QuarterName] varchar(9) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [Year] char(4) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [YearName] char(7) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [MonthYear] char(10) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [MMYYYY] char(6) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [FirstDayOfMonth] date NULL,\n    [LastDayOfMonth] date NULL,\n    [FirstDayOfQuarter] date NULL,\n    [LastDayOfQuarter] date NULL,\n    [FirstDayOfYear] date NULL,\n    [LastDayOfYear] date NULL,\n    [IsHolidayUSA] bit NULL,\n    [IsWeekday] bit NULL,\n    [HolidayUSA] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nCREATE TABLE [dbo].[Geography]\n(\n    [GeographyID] int NOT NULL,\n    [ZipCodeBKey] varchar(10) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,\n    [County] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [City] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [State] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [Country] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [ZipCode] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nCREATE TABLE [dbo].[HackneyLicense]\n(\n    [HackneyLicenseID] int NOT NULL,\n    [HackneyLicenseBKey] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,\n    [HackneyLicenseCode] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nCREATE TABLE [dbo].[Medallion]\n(\n    [MedallionID] int NOT NULL,\n    [MedallionBKey] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,\n    [MedallionCode] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nCREATE TABLE [dbo].[Time]\n(\n    [TimeID] int NOT NULL,\n    [TimeBKey] varchar(8) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,\n    [HourNumber] tinyint NOT NULL,\n    [MinuteNumber] tinyint NOT NULL,\n    [SecondNumber] tinyint NOT NULL,\n    [TimeInSecond] int NOT NULL,\n    [HourlyBucket] varchar(15) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,\n    [DayTimeBucketGroupKey] int NOT NULL,\n    [DayTimeBucket] varchar(100) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nCREATE TABLE [dbo].[Trip]\n(\n    [DateID] int NOT NULL,\n    [MedallionID] int NOT NULL,\n    [HackneyLicenseID] int NOT NULL,\n    [PickupTimeID] int NOT NULL,\n    [DropoffTimeID] int NOT NULL,\n    [PickupGeographyID] int NULL,\n    [DropoffGeographyID] int NULL,\n    [PickupLatitude] float NULL,\n    [PickupLongitude] float NULL,\n    [PickupLatLong] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [DropoffLatitude] float NULL,\n    [DropoffLongitude] float NULL,\n    [DropoffLatLong] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [PassengerCount] int NULL,\n    [TripDurationSeconds] int NULL,\n    [TripDistanceMiles] float NULL,\n    [PaymentType] varchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,\n    [FareAmount] money NULL,\n    [SurchargeAmount] money NULL,\n    [TaxAmount] money NULL,\n    [TipAmount] money NULL,\n    [TollsAmount] money NULL,\n    [TotalAmount] money NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\nCREATE TABLE [dbo].[Weather]\n(\n    [DateID] int NOT NULL,\n    [GeographyID] int NOT NULL,\n    [PrecipitationInches] float NOT NULL,\n    [AvgTemperatureFahrenheit] float NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "samplededicated",
						"poolName": "samplededicated"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/004_CopyStatementLoadfromBlob')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "DedicatedSQLPool"
				},
				"content": {
					"query": "COPY INTO [dbo].[Date]\nFROM 'https://nytaxiblob.blob.core.windows.net/2013/Date'\nWITH\n(\n    FILE_TYPE = 'CSV',\n\tFIELDTERMINATOR = ',',\n\tFIELDQUOTE = ''\n)\nOPTION (LABEL = 'COPY : Load [dbo].[Date] - Taxi dataset');\n\n\nCOPY INTO [dbo].[Geography]\nFROM 'https://nytaxiblob.blob.core.windows.net/2013/Geography'\nWITH\n(\n    FILE_TYPE = 'CSV',\n\tFIELDTERMINATOR = ',',\n\tFIELDQUOTE = ''\n)\nOPTION (LABEL = 'COPY : Load [dbo].[Geography] - Taxi dataset');\n\nCOPY INTO [dbo].[HackneyLicense]\nFROM 'https://nytaxiblob.blob.core.windows.net/2013/HackneyLicense'\nWITH\n(\n    FILE_TYPE = 'CSV',\n\tFIELDTERMINATOR = ',',\n\tFIELDQUOTE = ''\n)\nOPTION (LABEL = 'COPY : Load [dbo].[HackneyLicense] - Taxi dataset');\n\nCOPY INTO [dbo].[Medallion]\nFROM 'https://nytaxiblob.blob.core.windows.net/2013/Medallion'\nWITH\n(\n    FILE_TYPE = 'CSV',\n\tFIELDTERMINATOR = ',',\n\tFIELDQUOTE = ''\n)\nOPTION (LABEL = 'COPY : Load [dbo].[Medallion] - Taxi dataset');\n\nCOPY INTO [dbo].[Time]\nFROM 'https://nytaxiblob.blob.core.windows.net/2013/Time'\nWITH\n(\n    FILE_TYPE = 'CSV',\n\tFIELDTERMINATOR = ',',\n\tFIELDQUOTE = ''\n)\nOPTION (LABEL = 'COPY : Load [dbo].[Time] - Taxi dataset');\n\nCOPY INTO [dbo].[Weather]\nFROM 'https://nytaxiblob.blob.core.windows.net/2013/Weather'\nWITH\n(\n    FILE_TYPE = 'CSV',\n\tFIELDTERMINATOR = ',',\n\tFIELDQUOTE = '',\n\tROWTERMINATOR='0X0A'\n)\nOPTION (LABEL = 'COPY : Load [dbo].[Weather] - Taxi dataset');\n\nCOPY INTO [dbo].[Trip]\nFROM 'https://nytaxiblob.blob.core.windows.net/2013/Trip2013'\nWITH\n(\n    FILE_TYPE = 'CSV',\n\tFIELDTERMINATOR = '|',\n\tFIELDQUOTE = '',\n\tROWTERMINATOR='0X0A',\n\tCOMPRESSION = 'GZIP'\n)\nOPTION (LABEL = 'COPY : Load [dbo].[Trip] - Taxi dataset');",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "samplededicated",
						"poolName": "samplededicated"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/005_viewdataloads')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "DedicatedSQLPool"
				},
				"content": {
					"query": "SELECT  r.[request_id]                           \n,       r.[status]                               \n,       r.resource_class                         \n,       r.command\n,       sum(bytes_processed) AS bytes_processed\n,       sum(rows_processed) AS rows_processed\nFROM    sys.dm_pdw_exec_requests r\n              JOIN sys.dm_pdw_dms_workers w\n                     ON r.[request_id] = w.request_id\nWHERE [label] = 'COPY : Load [dbo].[Date] - Taxi dataset' OR\n    [label] = 'COPY : Load [dbo].[Geography] - Taxi dataset' OR\n    [label] = 'COPY : Load [dbo].[HackneyLicense] - Taxi dataset' OR\n    [label] = 'COPY : Load [dbo].[Medallion] - Taxi dataset' OR\n    [label] = 'COPY : Load [dbo].[Time] - Taxi dataset' OR\n    [label] = 'COPY : Load [dbo].[Weather] - Taxi dataset' OR\n    [label] = 'COPY : Load [dbo].[Trip] - Taxi dataset' \nand session_id <> session_id() and type = 'WRITER'\nGROUP BY r.[request_id]                           \n,       r.[status]                               \n,       r.resource_class                         \n,       r.command;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "samplededicated",
						"poolName": "samplededicated"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/006_viewsystemqueries')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "DedicatedSQLPool"
				},
				"content": {
					"query": "SELECT * FROM sys.dm_pdw_exec_requests;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "samplededicated",
						"poolName": "samplededicated"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/007_counts_objects_by_date_created')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "DedicatedSQLPool"
				},
				"content": {
					"query": "CREATE PROCEDURE count_objects_by_date_created \n                            @start_date DATETIME2,\n                            @end_date DATETIME2\nAS BEGIN \n\n    IF( @start_date >= GETUTCDATE() )\n    BEGIN\n        THROW 51000, 'Invalid argument @start_date. Value should be in past.', 1;  \n    END\n\n    IF( @end_date IS NULL )\n    BEGIN\n        SET @end_date = GETUTCDATE();\n    END\n\n    IF( @start_date >= @end_date )\n    BEGIN\n        THROW 51000, 'Invalid argument @end_date. Value should be greater than @start_date.', 2;  \n    END\n\n    SELECT\n         year = YEAR(create_date),\n         month = MONTH(create_date),\n         objects_created = COUNT(*)\n    FROM\n        sys.objects\n    WHERE\n        create_date BETWEEN @start_date AND @end_date\n    GROUP BY\n        YEAR(create_date), MONTH(create_date);\nEND",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "samplededicated",
						"poolName": "samplededicated"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/008_insert_medallian')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "DedicatedSQLPool"
				},
				"content": {
					"query": "CREATE PROCEDURE insert_Medallion\n                            @id  INT,\n                            @key NVARCHAR(255),\n                            @code NVARCHAR(255)\nAS BEGIN \n\n    insert into dbo.Medallion values (@id, @key, @code)\n    \nEND",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "samplededicated",
						"poolName": "samplededicated"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SQL_Serverless"
				},
				"content": {
					"query": "select * from tester234",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "samplededicated",
						"poolName": "samplededicated"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/00_setup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "pool002",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "902f4b73-a387-4cbb-b5b0-f5a810b6cb66"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourceGroups/mm-synapse-dev-rg/providers/Microsoft.Synapse/workspaces/mm-synapse-dev/bigDataPools/pool002",
						"name": "pool002",
						"type": "Spark",
						"endpoint": "https://mm-synapse-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pool002",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1. Get variables"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true,
							"tags": [
								"parameters"
							]
						},
						"source": [
							"keyvaultlsname = 'LS_KeyVault'\n",
							"adls2lsname = 'mm-synapse-dev-WorkspaceDefaultStorage'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 2. Linked Services Setup: KV and ADLS Gen2"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql import SparkSession\n",
							"\n",
							"sc = SparkSession.builder.getOrCreate()\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
							"storage_account = token_library.getSecretWithLS(keyvaultlsname, \"datalakeaccountname\")\n",
							"\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# spark.conf.set(\"spark.storage.synapse.linkedServiceName\", adls2lsname)\r\n",
							"# #spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")"
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# spark.conf.set(\"spark.storage.synapse.linkedServiceName\", adls2lsname)\r\n",
							"# #spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")"
						],
						"outputs": [],
						"execution_count": 65
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 3. Create Schemas"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"spark.sql(f\"CREATE SCHEMA IF NOT EXISTS dw LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data'\")\n",
							"spark.sql(f\"CREATE SCHEMA IF NOT EXISTS lnd LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data'\")\n",
							"spark.sql(f\"CREATE SCHEMA IF NOT EXISTS interim LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data'\")\n",
							"spark.sql(f\"CREATE SCHEMA IF NOT EXISTS malformed LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data'\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 4. Create Fact Tables"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"spark.sql(f\"DROP TABLE IF EXISTS dw.fact_parking\")\n",
							"\n",
							"spark.sql(f\"CREATE TABLE dw.fact_parking(dim_date_id STRING,dim_time_id STRING, dim_parking_bay_id STRING, dim_location_id STRING, dim_st_marker_id STRING, status STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/dw/fact_parking/'\")\n",
							" \n",
							"spark.sql(f\"REFRESH TABLE dw.fact_parking\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 5. Create Dimension Tables"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"spark.sql(f\"DROP TABLE IF EXISTS dw.dim_st_marker\")\n",
							"spark.sql(f\"CREATE TABLE dw.dim_st_marker(dim_st_marker_id STRING, st_marker_id STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/dw/dim_st_marker/'\")\n",
							"spark.sql(f\"REFRESH TABLE dw.dim_st_marker\")\n",
							" \n",
							"\n",
							"spark.sql(f\"DROP TABLE IF EXISTS dw.dim_location\")\n",
							"spark.sql(f\"CREATE TABLE dw.dim_location(dim_location_id STRING,lat FLOAT, lon FLOAT, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/dw/dim_location/'\")\n",
							"spark.sql(f\"REFRESH TABLE dw.dim_location\")\n",
							" \n",
							"\n",
							"spark.sql(f\"DROP TABLE IF EXISTS dw.dim_parking_bay\")\n",
							"spark.sql(f\"CREATE TABLE dw.dim_parking_bay(dim_parking_bay_id STRING, bay_id INT,`marker_id` STRING, `meter_id` STRING, `rd_seg_dsc` STRING, `rd_seg_id` STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/dw/dim_parking_bay/'\")\n",
							"spark.sql(f\"REFRESH TABLE dw.dim_parking_bay\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 6. Create dim date and time"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.functions import col\n",
							"spark.sql(f\"DROP TABLE IF EXISTS dw.dim_date\")\n",
							"spark.sql(f\"DROP TABLE IF EXISTS dw.dim_time\")\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 7. Create interim and error tables"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# DimDate\r\n",
							"\r\n",
							"dimdate = spark.read.csv(f\"abfss://datalake@{storage_account}.dfs.core.windows.net/data/seed/dim_date/dim_date.csv\", header=True)\r\n",
							"dimdate.write.saveAsTable(\"dw.dim_date\")\r\n",
							"\r\n",
							"# DimTime\r\n",
							"dimtime = spark.read.csv(f\"abfss://datalake@{storage_account}.dfs.core.windows.net/data/seed/dim_time/dim_time.csv\", header=True)\r\n",
							"dimtime.write.saveAsTable(\"dw.dim_time\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"spark.sql(f\"DROP TABLE IF EXISTS interim.parking_bay\")\n",
							"spark.sql(f\"CREATE TABLE interim.parking_bay(bay_id INT, `last_edit` TIMESTAMP, `marker_id` STRING, `meter_id` STRING, `rd_seg_dsc` STRING, `rd_seg_id` STRING, `the_geom` STRUCT<`coordinates`: ARRAY<ARRAY<ARRAY<ARRAY<DOUBLE>>>>, `type`: STRING>, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/interim/interim.parking_bay/'\")\n",
							"spark.sql(f\"REFRESH TABLE interim.parking_bay\")\n",
							"\n",
							"spark.sql(f\"DROP TABLE IF EXISTS interim.sensor\")\n",
							"spark.sql(f\"CREATE TABLE  interim.sensor(bay_id INT, `st_marker_id` STRING, `lat` FLOAT, `lon` FLOAT, `location` STRUCT<`coordinates`: ARRAY<DOUBLE>, `type`: STRING>, `status` STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/interim/interim.sensor/'\")\n",
							"spark.sql(f\"REFRESH TABLE  interim.sensor\")\n",
							"   \n",
							"\n",
							"spark.sql(f\"DROP TABLE IF EXISTS malformed.parking_bay\")\n",
							"spark.sql(f\"CREATE TABLE malformed.parking_bay(bay_id INT, `last_edit` TIMESTAMP,`marker_id` STRING, `meter_id` STRING, `rd_seg_dsc` STRING, `rd_seg_id` STRING, `the_geom` STRUCT<`coordinates`: ARRAY<ARRAY<ARRAY<ARRAY<DOUBLE>>>>, `type`: STRING>, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/malformed/malformed.parking_bay/'\")\n",
							"spark.sql(f\"REFRESH TABLE malformed.parking_bay\")\n",
							"\n",
							"spark.sql(f\"DROP TABLE IF EXISTS malformed.sensor\")\n",
							"spark.sql(f\"CREATE TABLE malformed.sensor(bay_id INT,`st_marker_id` STRING,`lat` FLOAT,`lon` FLOAT,`location` STRUCT<`coordinates`: ARRAY<DOUBLE>, `type`: STRING>,`status` STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/malformed/malformed.parking_bay/'\")\n",
							"spark.sql(f\"REFRESH TABLE malformed.sensor\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01a_explore')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d6ad2564-38f9-4bdf-a15b-60eec6cdcba4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"import os\n",
							"import datetime\n",
							"\n",
							"# For testing\n",
							"base_path = 'abfss://datalake@<YOUR_STORAGE_ACCOUNT>.dfs.core.windows.net/data/lnd/2021_XX_XX_X1_XX_XX'\n",
							"parkingbay_filepath = os.path.join(base_path, \"MelbParkingBayData.json\")\n",
							"sensors_filepath = os.path.join(base_path, \"MelbParkingSensorData.json\")\n",
							"\n",
							"\n",
							"parkingbay_sdf = spark.read\\\n",
							"  .option(\"multiLine\", True)\\\n",
							"  .json(parkingbay_filepath)\n",
							"sensordata_sdf = spark.read\\\n",
							"  .option(\"multiLine\", True)\\\n",
							"  .json(sensors_filepath)\n",
							"\n",
							"display(parkingbay_sdf)\n",
							"display(sensordata_sdf)\n",
							"display(sensordata_sdf)\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01b_explore_sqlserverless')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "pool002",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4e8a62fd-93fe-4558-ae83-4edbb60c84b5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourceGroups/mm-synapse-dev-rg/providers/Microsoft.Synapse/workspaces/mm-synapse-dev/bigDataPools/pool002",
						"name": "pool002",
						"type": "Spark",
						"endpoint": "https://mm-synapse-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pool002",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1. Get dynamic pipeline parameters"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"keyvaultlsname = 'Ls_KeyVault_01'\n",
							"db_user_key='synapseSQLPoolAdminUsername'\n",
							"db_pwd_key='synapseSQLPoolAdminPassword'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 2. Get username password from keyvault"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\n",
							"workspace_name=mssparkutils.env.getWorkspaceName()\n",
							"print(workspace_name)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 3. Get username password from keyvault"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Make sure user has been created through create_db_user.sql before reading the data\n",
							"sc = SparkSession.builder.getOrCreate()\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
							"username = token_library.getSecretWithLS(keyvaultlsname, db_user_key)\n",
							"password = token_library.getSecretWithLS(keyvaultlsname, db_pwd_key)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 4. Get Data from external Table"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Read Top 5 lines from the parking data\n",
							"hostname = f\"samplededicated.sql.azuresynapse.net\"\n",
							"print(hostname)\n",
							"port = 1433\n",
							"database = \"samplededicated\" \n",
							"jdbcUrl = f\"jdbc:sqlserver://{hostname}:{port};database={database}\"\n",
							"dbtable = \"dbo.siteinfo\"\n",
							"\n",
							"#Parking Data\n",
							"parking_data = spark.read \\\n",
							"    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
							"    .option(\"url\", jdbcUrl) \\\n",
							"    .option(\"dbtable\", dbtable) \\\n",
							"    .option(\"user\", username) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .load()\n",
							"print(parking_data.count())\n",
							"parking_data.show(5)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Read Top 5 lines from the sensor data\n",
							"dbtable = \"sensor_view\"\n",
							"sensor_data = spark.read \\\n",
							"    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
							"    .option(\"url\", jdbcUrl) \\\n",
							"    .option(\"dbtable\", dbtable) \\\n",
							"    .option(\"user\", username) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .load()\n",
							"print(sensor_data.count())\n",
							"sensor_data.show(5)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02_standardize')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "pool002",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "246d6e40-3765-466b-9355-4c6f86ef1da7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourceGroups/mm-synapse-dev-rg/providers/Microsoft.Synapse/workspaces/mm-synapse-dev/bigDataPools/pool002",
						"name": "pool002",
						"type": "Spark",
						"endpoint": "https://mm-synapse-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pool002",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1. Get dynamic pipeline parameters"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true,
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# Get folder where the REST downloads were placed\n",
							"infilefolder = '2021_10_05_07_58_15/'\n",
							"\n",
							"# Get pipeline name\n",
							"pipelinename = 'P_Ingest_MelbParkingData'\n",
							"\n",
							"# Get pipeline run id\n",
							"loadid = ''\n",
							"\n",
							"# Get keyvault linked service name\n",
							"keyvaultlsname = 'LS_KeyVault'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 2. Load file path variables"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true,
							"tags": []
						},
						"source": [
							"import os\n",
							"import datetime\n",
							"\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
							"\n",
							"# Primary storage info \n",
							"account_name = token_library.getSecretWithLS( keyvaultlsname, \"datalakeaccountname\")\n",
							"container_name = 'datalake' # fill in your container name \n",
							"relative_path = 'data/lnd/' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"load_id = loadid\n",
							"loaded_on = datetime.datetime.now()\n",
							"base_path = os.path.join(adls_path, infilefolder)\n",
							"\n",
							"parkingbay_filepath = os.path.join(base_path, \"MelbParkingBayData.json\")\n",
							"print(parkingbay_filepath)\n",
							"sensors_filepath = os.path.join(base_path, \"MelbParkingSensorData.json\")\n",
							"print(sensors_filepath)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 3. Transform: Standardize"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import ddo_transform.standardize as s\n",
							"\n",
							"# Retrieve schema\n",
							"parkingbay_schema = s.get_schema(\"in_parkingbay_schema\")\n",
							"sensordata_schema = s.get_schema(\"in_sensordata_schema\")\n",
							"\n",
							"# Read data\n",
							"parkingbay_sdf = spark.read\\\n",
							"  .schema(parkingbay_schema)\\\n",
							"  .option(\"badRecordsPath\", os.path.join(base_path, \"__corrupt\", \"MelbParkingBayData\"))\\\n",
							"  .json(parkingbay_filepath)\n",
							"sensordata_sdf = spark.read\\\n",
							"  .schema(sensordata_schema)\\\n",
							"  .option(\"badRecordsPath\", os.path.join(base_path, \"__corrupt\", \"MelbParkingSensorData\"))\\\n",
							"  .json(sensors_filepath)\n",
							"\n",
							"# Standardize\n",
							"t_parkingbay_sdf, t_parkingbay_malformed_sdf = s.standardize_parking_bay(parkingbay_sdf, load_id, loaded_on)\n",
							"t_sensordata_sdf, t_sensordata_malformed_sdf = s.standardize_sensordata(sensordata_sdf, load_id, loaded_on)\n",
							"\n",
							"# Insert new rows\n",
							"t_parkingbay_sdf.write.mode(\"append\").insertInto(\"interim.parking_bay\")\n",
							"t_sensordata_sdf.write.mode(\"append\").insertInto(\"interim.sensor\")\n",
							"\n",
							"# Insert bad rows\n",
							"t_parkingbay_malformed_sdf.write.mode(\"append\").insertInto(\"malformed.parking_bay\")\n",
							"t_sensordata_malformed_sdf.write.mode(\"append\").insertInto(\"malformed.sensor\")\n",
							"\n",
							"# Recording record counts for logging purpose\n",
							"parkingbay_count = t_parkingbay_sdf.count()\n",
							"sensordata_count = t_sensordata_sdf.count()\n",
							"parkingbay_malformed_count = t_parkingbay_malformed_sdf.count()\n",
							"sensordata_malformed_count = t_sensordata_malformed_sdf.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 4. Observability: Logging to Azure Application Insights using OpenCensus Library"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import logging\n",
							"import os\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
							"from opencensus.ext.azure.log_exporter import AzureEventHandler\n",
							"from datetime import datetime\n",
							"\n",
							"# Getting Application Insights instrumentation key\n",
							"appi_key = token_library.getSecretWithLS(keyvaultlsname,\"applicationInsightsKey\")\n",
							"\n",
							"# Enable App Insights\n",
							"aiLogger = logging.getLogger(__name__)\n",
							"aiLogger.addHandler(AzureEventHandler(connection_string = 'InstrumentationKey=' + appi_key))\n",
							"\n",
							"aiLogger.setLevel(logging.INFO)\n",
							"\n",
							"aiLogger.info(\"Standardize (ai): Started at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
							"properties = {\"custom_dimensions\": {\"pipeline\": pipelinename, \"run_id\": loadid, \"parkingbay_count\": parkingbay_count, \"sensordata_count\": sensordata_count, \"parkingbay_malformed_count\": parkingbay_malformed_count, \"sensordata_malformed_count\": sensordata_malformed_count}}\n",
							"aiLogger.info(\"Standardize (ai): Completed at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), extra=properties)\n",
							"\n",
							"# To query this log, go to the Azure Monitor and run the following kusto query (Scope: Application Insights instance):\n",
							"#customEvents\n",
							"#| order by timestamp desc\n",
							"#| project timestamp, appName, name,\n",
							"#    pipelineName             = customDimensions.pipeline,\n",
							"#    pipelineRunId            = customDimensions.run_id,\n",
							"#    parkingbayCount          = customDimensions.parkingbay_count,\n",
							"#    sensordataCount          = customDimensions.sensordata_count,\n",
							"#    parkingbayMalformedCount = customDimensions.parkingbay_malformed_count,\n",
							"#    sensordataMalformedCount = customDimensions.sensordata_malformed_count,\n",
							"#    dimParkingbayCount       = customDimensions.new_parkingbay_count\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 5. Observability: Logging to Log Analytics workspace using log4j"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import logging\n",
							"import sys\n",
							"\n",
							"# Enable Log Analytics using log4j\n",
							"log4jLogger = sc._jvm.org.apache.log4j\n",
							"logger = log4jLogger.LogManager.getLogger(\"ParkingSensorLogs\")\n",
							"\n",
							"def log(msg = ''):\n",
							"    env = mssparkutils.env\n",
							"    formatted_msg = f'Standardize (log4j): {msg}~{pipelinename}~{env.getJobId()}~{env.getPoolName()}~{env.getWorkspaceName()}~{env.getUserId()}'\n",
							"    logger.info(formatted_msg)\n",
							"\n",
							"log(\"Started at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
							"\n",
							"log(f'parkingbay_count: {parkingbay_count}')\n",
							"log(f'sensordata_count: {sensordata_count}')\n",
							"log(f'parkingbay_malformed_count: {parkingbay_malformed_count}')\n",
							"log(f'sensordata_malformed_count: {sensordata_malformed_count}')\n",
							"\n",
							"log(\"Completed at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
							"\n",
							"# To query this log, go to the log analytics workspace and run the following kusto query (Scope: Log Analytics Workspace):\n",
							"#SparkLoggingEvent_CL\n",
							"#| where logger_name_s == \"ParkingSensorLogs\"\n",
							"#| order by TimeGenerated desc\n",
							"#| project TimeGenerated, workspaceName_s, Level,\n",
							"#    message         = split(Message, '~', 0),\n",
							"#    pipelineName    = split(Message, '~', 1),\n",
							"#    jobId           = split(Message, '~', 2),\n",
							"#    SparkPoolName   = split(Message, '~', 3),\n",
							"#    UserId          = split(Message, '~', 5)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03_transform')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "pool001",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "b5e46b4c-ae0d-41b8-82bb-aee3152d8a84"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourceGroups/mm-synapse-dev-rg/providers/Microsoft.Synapse/workspaces/mm-synapse-dev/bigDataPools/pool001",
						"name": "pool001",
						"type": "Spark",
						"endpoint": "https://mm-synapse-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pool001",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1. Get dynamic pipeline parameters"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true,
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# Get pipeline name\n",
							"pipelinename = 'pipeline_name'\n",
							"\n",
							"# Get pipeline run id\n",
							"loadid = ''\n",
							"\n",
							"# Get keyvault linked service name\n",
							"keyvaultlsname = 'Ls_KeyVault_01'"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 2. Transform and load Dimension tables\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import datetime\n",
							"import os\n",
							"from pyspark.sql.functions import col, lit\n",
							"import ddo_transform.transform as t\n",
							"import ddo_transform.util as util\n",
							"\n",
							"load_id = loadid\n",
							"loaded_on = datetime.datetime.now()\n",
							"\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
							"\n",
							"# Primary storage info \n",
							"account_name = token_library.getSecretWithLS(keyvaultlsname,\"datalakeaccountname\")\n",
							"container_name = 'datalake' # fill in your container name \n",
							"relative_path = 'data/dw/' # fill in your relative folder path \n",
							"\n",
							"base_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"\n",
							"# Read interim cleansed data\n",
							"parkingbay_sdf = spark.read.table(\"interim.parking_bay\").filter(col('load_id') == lit(load_id))\n",
							"sensordata_sdf = spark.read.table(\"interim.sensor\").filter(col('load_id') == lit(load_id))\n",
							"\n",
							"# Read existing Dimensions\n",
							"dim_parkingbay_sdf = spark.read.table(\"dw.dim_parking_bay\")\n",
							"dim_location_sdf = spark.read.table(\"dw.dim_location\")\n",
							"dim_st_marker = spark.read.table(\"dw.dim_st_marker\")\n",
							"\n",
							"# Transform\n",
							"new_dim_parkingbay_sdf = t.process_dim_parking_bay(parkingbay_sdf, dim_parkingbay_sdf, load_id, loaded_on).cache()\n",
							"new_dim_location_sdf = t.process_dim_location(sensordata_sdf, dim_location_sdf, load_id, loaded_on).cache()\n",
							"new_dim_st_marker_sdf = t.process_dim_st_marker(sensordata_sdf, dim_st_marker, load_id, loaded_on).cache()\n",
							"\n",
							"# Load\n",
							"util.save_overwrite_unmanaged_table(spark, new_dim_parkingbay_sdf, table_name=\"dw.dim_parking_bay\", path=os.path.join(base_path, \"dim_parking_bay\"))\n",
							"util.save_overwrite_unmanaged_table(spark, new_dim_location_sdf, table_name=\"dw.dim_location\", path=os.path.join(base_path, \"dim_location\"))\n",
							"util.save_overwrite_unmanaged_table(spark, new_dim_st_marker_sdf, table_name=\"dw.dim_st_marker\", path=os.path.join(base_path, \"dim_st_marker\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 3. Transform and load Fact tables"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Read existing Dimensions\n",
							"dim_parkingbay_sdf = spark.read.table(\"dw.dim_parking_bay\")\n",
							"dim_location_sdf = spark.read.table(\"dw.dim_location\")\n",
							"dim_st_marker = spark.read.table(\"dw.dim_st_marker\")\n",
							"\n",
							"# Process\n",
							"new_fact_parking = t.process_fact_parking(sensordata_sdf, dim_parkingbay_sdf, dim_location_sdf, dim_st_marker, load_id, loaded_on)\n",
							"\n",
							"# Insert new rows\n",
							"new_fact_parking.write.mode(\"append\").insertInto(\"dw.fact_parking\")\n",
							"\n",
							"# Recording record counts for logging purpose\n",
							"new_dim_parkingbay_count = spark.read.table(\"dw.dim_parking_bay\").count()\n",
							"new_dim_location_count = spark.read.table(\"dw.dim_location\").count()\n",
							"new_dim_st_marker_count = spark.read.table(\"dw.dim_st_marker\").count()\n",
							"new_fact_parking_count = new_fact_parking.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 4. Observability: Logging to Azure Application Insights using OpenCensus Library"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import logging\n",
							"import os\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
							"from opencensus.ext.azure.log_exporter import AzureEventHandler\n",
							"from datetime import datetime\n",
							"\n",
							"# Getting Application Insights instrumentation key\n",
							"appi_key = token_library.getSecretWithLS(keyvaultlsname,\"applicationInsightsKey\")\n",
							"\n",
							"# Enable App Insights\n",
							"aiLogger = logging.getLogger(__name__)\n",
							"aiLogger.addHandler(AzureEventHandler(connection_string = 'InstrumentationKey=' + appi_key))\n",
							"\n",
							"aiLogger.setLevel(logging.INFO)\n",
							"\n",
							"aiLogger.info(\"Transform (ai): Started at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
							"properties = {\"custom_dimensions\": {\"pipeline\": pipelinename, \"run_id\": loadid, \"new_parkingbay_count\": new_dim_parkingbay_count}}\n",
							"aiLogger.info(\"Transform (ai): Completed at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), extra=properties)\n",
							"\n",
							"# To query this log, go to the Azure Monitor and run the following kusto query (Scope: Application Insights instance):\n",
							"#customEvents\n",
							"#| order by timestamp desc\n",
							"#| project timestamp, appName, name,\n",
							"#    pipelineName             = customDimensions.pipeline,\n",
							"#    pipelineRunId            = customDimensions.run_id,\n",
							"#    parkingbayCount          = customDimensions.parkingbay_count,\n",
							"#    sensordataCount          = customDimensions.sensordata_count,\n",
							"#    parkingbayMalformedCount = customDimensions.parkingbay_malformed_count,\n",
							"#    sensordataMalformedCount = customDimensions.sensordata_malformed_count,\n",
							"#    dimParkingbayCount       = customDimensions.new_parkingbay_count"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 5. Observability: Logging to Log Analytics workspace using log4j"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import logging\n",
							"import sys\n",
							"\n",
							"# Enable Log Analytics using log4j\n",
							"log4jLogger = sc._jvm.org.apache.log4j\n",
							"logger = log4jLogger.LogManager.getLogger(\"ParkingSensorLogs\")\n",
							"\n",
							"def log(msg = ''):\n",
							"    env = mssparkutils.env\n",
							"    formatted_msg = f'Transform (log4j): {msg}~{pipelinename}~{env.getJobId()}~{env.getPoolName()}~{env.getWorkspaceName()}~{env.getUserId()}'\n",
							"    logger.info(formatted_msg)\n",
							"\n",
							"log(\"Started at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
							"\n",
							"log(f'new_dim_parkingbay_count: {new_dim_parkingbay_count}')\n",
							"log(f'new_dim_location_count: {new_dim_location_count}')\n",
							"log(f'new_dim_st_marker_count: {new_dim_st_marker_count}')\n",
							"log(f'new_fact_parking_count: {new_fact_parking_count}')\n",
							"\n",
							"log(\"Completed at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
							"\n",
							"# To query this log, go to the log analytics workspace and run the following kusto query (Scope: Log Analytics Workspace):\n",
							"#SparkLoggingEvent_CL\n",
							"#| where logger_name_s == \"ParkingSensorLogs\"\n",
							"#| order by TimeGenerated desc\n",
							"#| project TimeGenerated, workspaceName_s, Level,\n",
							"#    message         = split(Message, '~', 0),\n",
							"#    pipelineName    = split(Message, '~', 1),\n",
							"#    jobId           = split(Message, '~', 2),\n",
							"#    SparkPoolName   = split(Message, '~', 3),\n",
							"#    UserId          = split(Message, '~', 5)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3105faa7-6e2f-4c6d-a1c3-686f4c462013"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://datalake@mmxsynapsexdev.dfs.core.windows.net/data/seed/dim_date/dim_date.csv', format='csv'\r\n",
							"## If header exists uncomment line below\r\n",
							"##, header=True\r\n",
							")\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "pool001",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "8d46503a-0f9b-491e-8ffd-7b581010851a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourceGroups/mm-synapse-dev-rg/providers/Microsoft.Synapse/workspaces/mm-synapse-dev/bigDataPools/pool001",
						"name": "pool001",
						"type": "Spark",
						"endpoint": "https://mm-synapse-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pool001",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import ddo_transform.standardize as s"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pool001')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"libraryRequirements": {
					"content": "opencensus==0.7.13\n",
					"filename": "requirements.txt",
					"time": "2022-06-29T17:56:26.2016704Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"customLibraries": [
					{
						"name": "ddo_transform-localdev-py2.py3-none-any.whl",
						"path": "mm-synapse-dev/libraries/ddo_transform-localdev-py2.py3-none-any.whl",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "whl"
					}
				],
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pool002')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"libraryRequirements": {
					"content": "opencensus==0.7.13\n",
					"filename": "requirements.txt",
					"time": "2022-06-29T17:56:43.3405146Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"customLibraries": [
					{
						"name": "ddo_transform-localdev-py2.py3-none-any.whl",
						"path": "mm-synapse-dev/libraries/ddo_transform-localdev-py2.py3-none-any.whl",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "whl"
					}
				],
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/samplededicated')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}